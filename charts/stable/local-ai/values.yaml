image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.1@sha256:77210d1c106349b765a9336d747327eb5c66f23cec02c0dafd83deb972030e27
ffmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.0-ffmpeg-core@sha256:612c69f08e2012a3b7b1c7d73e03b482cb73891b7d6a7d1872c6db99d2335c50
cublasCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.1-cublas-cuda12-core@sha256:523cdd0ad3d5ee0f1b264f9c685aaffd7ba3c270c4ff376af7e5d2f004653875
cublasCuda12FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.0-cublas-cuda12-ffmpeg-core@sha256:d820afd3b708b4c2256536a1185bf6ee09c0e1a7f40f79154f78444c1874dde9
cublasCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.1-cublas-cuda11-core@sha256:210ed4461e454ed339dc0a80c7064b56c937e4963bacd85118fe7c8552ee68b3
cublasCuda11FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.0-cublas-cuda11-ffmpeg-core@sha256:97a44c583b51a8916ef8a7d98e003238d58f1d97974b4f828d5eba7c019518eb
allInOneCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.0-aio-gpu-nvidia-cuda-12@sha256:d9343dfee13796666d2ff7725afeb6440742ced3091d9e2521b01f3c4baa624c
allInOneCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.0-aio-gpu-nvidia-cuda-11@sha256:4cc7db08beaaee87f0eca31ff1395c38ce9e06e562640c247fdadc3956ce57af
allInOneCpuImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.21.0-aio-cpu@sha256:b4e5c8507c4b1aef84d2367ab142b7fc0519ccac9e8bdbb2965d5abb6a5001de
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0
service:
  main:
    ports:
      main:
        protocol: http
        port: 8080
localai:
  # Specify a build type. Available: cublas, openblas, clblas.
  build_type: "openblas"
  debug: false
  cors: true
  cors_allow_origins: "*"
  galleries: []
  #  - name: model-gallery
  #    url: github:go-skynet/model-gallery/index.yaml
  preload_models: []
  #    url: github:go-skynet/model-gallery/gpt4all-j.yaml
  # UPLOAD_LIMIT
workload:
  main:
    podSpec:
      containers:
        main:
          probes:
            liveness:
              enabled: true
              type: http
              path: /readyz
            readiness:
              enabled: true
              type: http
              path: /readyz
            startup:
              enabled: true
              type: tcp
          imageSelector: image
          env:
            ADDRESS: ":{{ .Values.service.main.ports.main.port }}"
            MODELS_PATH: "{{ .Values.persistence.models.mountPath }}"
            IMAGE_PATH: "{{ .Values.persistence.images.mountPath }}"
            BUILD_TYPE: "{{ .Values.localai.build_type }}"
            # breaks chart if true, keep it false.
            REBUILD: false
            DEBUG: "{{ .Values.localai.debug }}"
            CORS: "{{ .Values.localai.cors }}"
            GALLERIES: "{{ toJson .Values.localai.galleries }}"
            PRELOAD_MODELS: "{{ toJson .Values.localai.preload_models }}"
            CORS_ALLOW_ORIGINS: "{{ .Values.localai.cors_allow_origins }}"
persistence:
  models:
    enabled: true
    mountPath: "/models"
  images:
    enabled: true
    mountPath: "/images"
portal:
  open:
    enabled: false
updated: true
