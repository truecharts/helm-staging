image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3@sha256:0cb22d95e97831288d3b5401bf59ec041c476f1dfca2157b4e13bb22de4156b0
ffmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-ffmpeg-core@sha256:f7005e8a3371ad4cba3a2b40595ed1d405f6a0b22cf96135030923dc272d67db
cublasCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-cublas-cuda12-core@sha256:fd29bc6c4c43a326b9128104594400897c50f3f2ad95e7d16180fe1771c58d83
cublasCuda12FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-cublas-cuda12-ffmpeg-core@sha256:c4feed027ae449a6dd069a718e6b82cdb8b665935d228cf5e2c5c7973c1a34c6
cublasCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-cublas-cuda11-core@sha256:8297c5f353f19bbc58dde3d1adf2d5d829c779e3bf74a482b729a4833fc9219b
cublasCuda11FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-cublas-cuda11-ffmpeg-core@sha256:4bd482bd5275d8fe31c948f831f4fcd6063513a6ebbb5dd7d35de415e3874fdf
allInOneCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-aio-gpu-nvidia-cuda-12@sha256:87b6ce3d254084aedf6c015a1cc668e18f607eac762e5a582f6960b65c407e39
allInOneCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-aio-gpu-nvidia-cuda-11@sha256:c0f529e60b371f2442e6cb965085773a5e838a5a86ef16d439cef68f86537b60
allInOneCpuImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.19.3-aio-cpu@sha256:0d6d6cb8366f92276d1eab8fe5b7a31346e6c6b2adfa7407aed5530d54d42898
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0
service:
  main:
    ports:
      main:
        protocol: http
        port: 8080
localai:
  # Specify a build type. Available: cublas, openblas, clblas.
  build_type: "openblas"
  debug: false
  cors: true
  cors_allow_origins: "*"
  galleries: []
  #  - name: model-gallery
  #    url: github:go-skynet/model-gallery/index.yaml
  preload_models: []
  #    url: github:go-skynet/model-gallery/gpt4all-j.yaml
  # UPLOAD_LIMIT
workload:
  main:
    podSpec:
      containers:
        main:
          probes:
            liveness:
              enabled: true
              type: http
              path: /readyz
            readiness:
              enabled: true
              type: http
              path: /readyz
            startup:
              enabled: true
              type: tcp
          imageSelector: image
          env:
            ADDRESS: ":{{ .Values.service.main.ports.main.port }}"
            MODELS_PATH: "{{ .Values.persistence.models.mountPath }}"
            IMAGE_PATH: "{{ .Values.persistence.images.mountPath }}"
            BUILD_TYPE: "{{ .Values.localai.build_type }}"
            # breaks chart if true, keep it false.
            REBUILD: false
            DEBUG: "{{ .Values.localai.debug }}"
            CORS: "{{ .Values.localai.cors }}"
            GALLERIES: "{{ toJson .Values.localai.galleries }}"
            PRELOAD_MODELS: "{{ toJson .Values.localai.preload_models }}"
            CORS_ALLOW_ORIGINS: "{{ .Values.localai.cors_allow_origins }}"
persistence:
  models:
    enabled: true
    mountPath: "/models"
  images:
    enabled: true
    mountPath: "/images"
portal:
  open:
    enabled: false
updated: true
